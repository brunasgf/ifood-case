{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f7pmSQksHQBZ"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from pyspark.sql import SparkSession, DataFrame\n",
        "from pyspark.sql.functions import col\n",
        "from pyspark.sql.types import IntegerType, DoubleType, TimestampType\n",
        "\n",
        "\n",
        "class SilverLayerBuilder:\n",
        "    \"\"\"\n",
        "    Constrói a Silver Layer a partir de diretórios brutos de Parquet (yellow/green),\n",
        "    normalizando esquemas e salvando no caminho de destino.\n",
        "    \"\"\"\n",
        "    def __init__(self, spark: SparkSession, base_dir: str):\n",
        "        self.spark = spark\n",
        "        self.base_dir = base_dir.rstrip(\"/\")\n",
        "        self.raw_roots = {\n",
        "            \"yellow\": os.path.join(self.base_dir, \"yellow\"),\n",
        "            \"green\":  os.path.join(self.base_dir, \"green\"),\n",
        "        }\n",
        "        self.silver_base = os.path.join(self.base_dir, \"silver_layer\")\n",
        "\n",
        "    def list_parquet_files(self, root_path: str) -> list[str]:\n",
        "        \"\"\"Recusa todos os arquivos .parquet em root_path (recursivamente).\"\"\"\n",
        "        files = []\n",
        "        for dirpath, _, filenames in os.walk(root_path):\n",
        "            for fn in filenames:\n",
        "                if fn.endswith(\".parquet\"):\n",
        "                    files.append(os.path.join(dirpath, fn))\n",
        "        return sorted(files)\n",
        "\n",
        "    def read_and_cast(self, path: str, is_green: bool=False) -> DataFrame:\n",
        "        \"\"\"\n",
        "        Lê um Parquet, renomeia colunas de datetime se for green,\n",
        "        faz cast para tipos coerentes e retorna apenas as colunas alvo.\n",
        "        \"\"\"\n",
        "        df = self.spark.read.parquet(path)\n",
        "\n",
        "        if is_green:\n",
        "            df = (\n",
        "                df\n",
        "                .withColumnRenamed(\"lpep_pickup_datetime\",  \"tpep_pickup_datetime\")\n",
        "                .withColumnRenamed(\"lpep_dropoff_datetime\", \"tpep_dropoff_datetime\")\n",
        "            )\n",
        "\n",
        "        return (\n",
        "            df\n",
        "            .withColumn(\"VendorID\",             col(\"VendorID\").cast(IntegerType()))\n",
        "            .withColumn(\"passenger_count\",      col(\"passenger_count\").cast(IntegerType()))\n",
        "            .withColumn(\"total_amount\",         col(\"total_amount\").cast(DoubleType()))\n",
        "            .withColumn(\"tpep_pickup_datetime\", col(\"tpep_pickup_datetime\").cast(TimestampType()))\n",
        "            .withColumn(\"tpep_dropoff_datetime\",col(\"tpep_dropoff_datetime\").cast(TimestampType()))\n",
        "            .select(\n",
        "                \"VendorID\",\n",
        "                \"passenger_count\",\n",
        "                \"total_amount\",\n",
        "                \"tpep_pickup_datetime\",\n",
        "                \"tpep_dropoff_datetime\",\n",
        "            )\n",
        "        )\n",
        "\n",
        "    def build(self):\n",
        "        \"\"\"Executa o processo de leitura, normalização e gravação da Silver Layer.\"\"\"\n",
        "        for tipo, raw_root in self.raw_roots.items():\n",
        "            is_green = (tipo == \"green\")\n",
        "            parquet_files = self.list_parquet_files(raw_root)\n",
        "\n",
        "            if not parquet_files:\n",
        "                print(f\"Nenhum Parquet encontrado em: {raw_root}\")\n",
        "                continue\n",
        "\n",
        "            df_acc = None\n",
        "            for p in parquet_files:\n",
        "                df_piece = self.read_and_cast(p, is_green=is_green)\n",
        "                df_acc = df_piece if df_acc is None else df_acc.unionByName(df_piece)\n",
        "\n",
        "            out_path = os.path.join(self.silver_base, tipo)\n",
        "            df_acc.write.mode(\"overwrite\").parquet(out_path)\n",
        "            print(f\"Silver layer ({tipo}) salva em: {out_path}\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    spark = SparkSession.builder \\\n",
        "        .appName(\"SilverLayerBuilder\") \\\n",
        "        .getOrCreate()\n",
        "\n",
        "    base_dir = \"/content/drive/MyDrive/ifood/teste_2\"\n",
        "    builder = SilverLayerBuilder(spark, base_dir)\n",
        "    builder.build()\n",
        "\n",
        "    spark.stop()\n"
      ],
      "metadata": {
        "id": "SLZmhO2hHx-L"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}